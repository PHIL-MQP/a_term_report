\documentclass{article}
\usepackage[margin=1.5in]{geometry}
\usepackage{float}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{amsmath}
\usepackage{nameref}
\usepackage{siunitx}
\usepackage[utf8]{inputenc}

\begin{document}

\title{Localization for FRC \\
  \large{A Term Report}
  }
\author{Jinan (Dorothy) Hu, Peter Mitrano, Kacper Puczydlowski, Nicolette Vere}

\maketitle{}

\section{Introduction}

Knowing the position and orientation of a mobile robot is critical to many tasks. For robots designed for high-speed gameplay, knowing the position and orientation allows the robot to perform complex autonomous behaviors such as shooting and retrieving game objects. In this report, we describe a system for determining the pose $(x, y, \theta)$ of a mobile robot in a cluttered environment. The environment we are interested in is the FIRST Robotics Competition (FRC). FRC is a challenging environment because the robots make rapid and aggressive maneuvers by human drivers for part of the time, and at other times are under complete autonomy. A successful localization system for FRC must handle up to six robots, occlusion from the playing field elements, unpredictable lighting, and frequent impacts. Our research suggests that there are at least five appropriate methods for localization: cameras and tags, radio and ultrasonic beacons, optical flow, and dead reckoning with encoders, and dead reckoning with an IMU. All of these methods have seen success in robot localization.

This report begins with a review of existing literature on indoor localization for mobile robots in the \nameref{related_work} section. The strengths and weaknesses of these existing methods are described in the \nameref{methods} section. The \nameref{experiments} section contains information on experiments we conducted, and the \nameref{conclusion} section details our plan moving forward.

\section{Related Work} \label{related_work}

Robot localization has been studied for decades. Overall, the problem of localizing a mobile robot can be viewed as accurately measure the absolute distance to known landmarks, or by measuring the changes in position over time. We will henceforth refer to these two ideas as global and local pose estimation. Some of the high level techniques for robot localization are: measuring range at various points around the robot and matching these readings to a map, measuring time of flight or difference of time of arrival to calculate distance to known locations, recognizing landmarks in some modality and computing pose relative to those landmarks, and measuring changes in pose and accumulating these changes over time. There are different sensors that can be used for each of these techniques, such as laser range finders, ultrasonic, cameras, inertial measurement units (IMU), encoders, radio, visible light, and human-audible sound. Although there are a tremendous number of possible methods for robot localization, there are a few which have received the most attention and shown the most success. These include:
\begin{itemize}
    \item Lidar mapping
    \item Ultrasonic mappping
    \item IMU and Encoders fusion
    \item Infrared or Radio and Ultrasonic beacons
    \item cameras with visually identifiable tags
    \item Optical flow mice and cameras
    \item Stereo vision and depth cameras
\end{itemize}

In our research, we learned how these seven techniques work and found descriptions and implementations in order to evaluate them. These descriptions and implementations are described in this section with the purpose of demonstrating a thorough literature review and of providing background information to the reader.

An inertial measurement unit (IMU) is a sensor reporting the forces acting upon an object, the angular rates of change, and surrounding magnetic field. The device typically comprises an accelerometer, gyroscope, and magnetometer which sense the above data respectively. These devices function by detecting Coriolis forces, inertial forces acting in the direction of motion relative to a rotating reference frame. These forces are proportional to the magnitude of the acceleration. These forces may be detected by a simple strain gauge mechanism or object vibrating at a know frequency ( the rate of change of vibration frequency is detected) \cite{THIS_REFERENCE_IS_ON_OTHER_COMPUTER_SAD_TIMES}. The premise behind position sensing using this device involves integrating the data with respect to time to calculate position and orientation. This approach was first used in aeronautics to estimate projectile attitude, orientation, and position \cite{agard_1989}. High cost IMU's have been used historically for defense and transportation systems; the quality of the sensor is high and the data is reliable in these applications. An inertial navigation system (INS) often comprises multiple accelerometers, gyroscopes, and magnetometers to estimate orientation and position. Their performance is increased by referencing, or filtering, one sensor to estimate the error from another. Simple double integration of a filtered system using expensive sensors is often sufficient for position tracking applications like ballistics or missile tracking \cite{}.

In cost-sensitive systems, this methodology is subject to high error rates from accumulation. Because of integration of accelerometer data, the velocity error term grows linearly and position error quadratically. This introduces a need for sensor fusion and optimization based approaches.

Rarely in mobile applications are IMUs the primary position sensor. Odometers and encoders offer relative position sensing because they measure change in position, but must be provided with a frame of reference. Cameras, radio beacons, GPS, and similar landmark-based technologies offer global position sensing; they inherently do this when reporting position because they detect a know indicator or reference a globally known position.

Often, IMU data is continuously corrected using another sensor, such as an odometer or global positioning system. These sensors are subject to drift and bias.

**Here we will go over all the papers we read and sources we talked to. Explain what if any of our approach is novel and what is built on existing approaches.

Beacon systems have been used many times with success in the literature. Generally, these systems ultrasound and or radio as a medium and either signal strength, phase shift, or time to measure distance to the beacons. Among radio systems, the system in \cite{bahl_radar:_2000} identified the location of people moving around buildings using signal strength in the 2.4gHz band received at three or more beacons, and they report accuracy of a few meters with an update rate of at most four times per second. The systems described in \cite{digiampaolo_mobile_2014} uses passive RFID tags on the ceiling and an RFID transmitter on the robot, and report an accuracy of 4\si{\centi\meter} within a 5\si{\square\meter}. Another RFID system \cite{saab_standalone_2011} also uses signal strength to RFID, and reports accuracies for various configurations ranging from 1\si{\centi\meter} to 3\si{\meter}. These RFID systems require readers that cost over \$500. There are also countless localization systems that use standard wireless networks. A comprehensive survey of these systems can be found in \cite{liu_survey_2007}. Systems that use signal strength in standard wireless LAN networks have achieved up to 10\si{\centi\meter} accuracy and hundreds of updates per second. Another radio beacon solution is to substitute single-frequency radio with Ultra-wideband radio. These systems can achieve centimeter level accuracy, but they use obscure or custom made transmitters and receivers that cost in the hundreds of dollars \cite{noauthor_dart_nodate} \cite{noauthor_pozyx_nodate}. Among ultrasonic beacon systems, \cite{kleeman_optimal_1992} uses the raw arrival times of ultrasonic pulses over time plus odometry together in a Kalman filter. Many beacon systems use the speed difference between sound and electromagnetic waves to measure system. Systems like \cite{smith_tracking_2004} \cite{ward_new_1997} \cite{kim_advanced_2008} send radio pulses followed by ultrasonic pulses. Nodes in the network us the difference in arrival time of these two signals to measure distance. Alternately, some systems use infrared pulses in place of radio \cite{ghidary_new_1999} \cite{yucel_development_2012}. These systems are inexpensive, and report accuracy of between 2 and 14\si{\centi\meter}.

\section{Evaluation of Localization Techniques} \label{methods}

Each of the techniques presented thus far have strengths and weaknesses, and it is unlikely that any one technique would be sufficient. This is the motivation for combining multiple techniques. To do so effectively, we compare each technique so as to mitigate the errors in any one technique. The five

Optical flow gives us accurate angle measurements and fast updates that are relative to our current position. Like all camera based solutions, the vibration of the robot will likely makes this technique difficult. However, cameras are the most widely used sensor according to our survey of FRC students and alumni.
The camera also doubles as a sensor for matrix tags, which give us accurate position estimates in the global frame. This is complemented by beacons, which update more slowly, but are not effected by occlusion and is robust to vibration.

\section{Experimental Results} \label{experiments}

**How did we test ArUco tags, IMUs, Optical Flow. What results? Be specific. Include any relevant charts or equations.

We demonstrated radio communication between two microcontrollers and tested the effect of distance and occlusion.

\section{Conclusion} \label{conclusion}

Based on the extensive literature review and the few preliminary experiments we've conducted, we eliminated our initial list of possible techniques down to the following five:
\begin{enumerate}
    \item Radio and ultrasonic beacons
    \item IMU
    \item Drive wheel encoders
    \item Optical flow
    \item Camera with matrix tags
\end{enumerate}

We have found examples of each of these techniques being used successfully, and in many cases have verified that they satisfy our criteria for accuracy, precision, update rate, and accessibility criteria. We are confident that any combination of these methods would work. Nonetheless, it is unreasonable to attempt to use all of these methods in the time frame of this project, and therefore we have decided to move forward with beacons, optical flow, and matrix tags. These techniques are complementary in their sources of error, and together we believe they will make a robust localization system.

\subsection{Proposed System Specification}
Having decided on the sensing techinques we will use, we now describe our detailed system Specification. Ideally, the description in this section serves as the full plan for our implementation of the system during B Term, and most of the major design decisions have been made and clearly presented.

\subsubsection{Radio and Ultrasonic Beacons}

First, we consider how many beacons we will need. This will determined by costs and by ultrasonic or radio performance characteristics.

\textbf{Format for Ultrasonic Signal} \\
The ultrasonic signal should be designed to maximize both the distance it can be detected from and accuracy with which it's timing can be detected. Using a simple pulse of fixed frequency and amplitude is not ideal because it trades off energy transmitted (and thus distance) with accuracy. This is because a longer pulse contained more energy and can be detected from further, but at the same time the receiver cannot distinguish timing within the length of the pulse. Therefore, a chirp signal will be used. Compare the usual function relating amplitude to time of a sine wave to the quadtric chirp signal.
$$ x(t) = A\cos(\omega t + \phi) $$
$$ x(t) = A\cos\bigg(2\pi\Big(\tfrac{f_1 - f_0}{2T}t^2+f_0t\Big) + \phi\bigg) $$

This function will generate a chirp with amplitude $A$, starting from frequency $f_0$ going to $f_1$ over time interval $T$. Figure \ref{fig:chirp} shows the specific waveform with the parameters we expect to use. This is a linear chirp signal because the instanteous frequency of the signal changes linearly with time.

\begin{figure}[H]
  \centering
  \begin{tikzpicture}
    \begin{axis}[width=12cm,height=6cm]
      \addplot[domain=-.0002:0,samples=10]{0};
      \addplot[domain=0:0.001,samples=1000]{sin((2*180*((27000 - 20000)/(2*0.001)*x*x + 20000*x)))};
      \addplot[domain=0.001:0.0012,samples=10]{0};
    \end{axis}
  \end{tikzpicture}
  \label{fig:chirp}
  \caption{Chirp signal with $\phi=0$, $A=1$, $f_0=\SI{20}{\kilo\hertz}$, $f_1=\SI{27}{\kilo\hertz}$, and $T=\SI{1}{\milli\second}$}
\end{figure}

This is the waveform we will be emitting from the ultrsonic sensors. The benefit is that it can be high power and still allow the receiver to determine precisely where in the waveform it is listening. To do this, one can take the discrete short time forier transform, which will show us how the power at various frequencies changes over time. By applying the FFT at a particular instant during the chirp, we can determine the position in time within the chirp of the current FFT. For instance, in the signal above if we apply and FFT at time $t_n=1.0$ and discover high power at frequency $f=\SI{22}{\kilo\hertz}$, we know that the chirp began at time $t_0 = t_n - \tfrac{f - f_0}{f_1 - f_0}T = 1.0 - \tfrac{22-20}{27-20}0.001 = 0.99857$. \\

Our beacon system relies on accurately knowing the distance to a beacon, and therefore knowing exactly when the start of the signal left the transmitter and when the start of the signal arrived at the receiver. Figure \ref{fig:rx_tx_timing} shows the sources of delay we are accounting for. Using the procedure above, we can calculate when the start of the signal arrived at the transmitter for ultrasonic.

\begin{figure}
  \centering
  \begin{tikzpicture}
    % timeline for ultrasonic
    \draw (-1.5, 0) node {ULTRASONIC};
    \filldraw (0,0) circle (0.05);
    \draw (0.5, -0.2) node {\tiny TX Delay};
    \draw (0,0) -- (1,0);
    \filldraw (1,0) circle (0.05);
    \draw (5.5, -0.2) node {\tiny Time of flight};
    \draw (1,0) -- (10,0);
    \filldraw (10,0) circle (0.05);
    \draw (10.5, -0.2) node {\tiny RX Delay};
    \draw (10,0) -- (11,0);
    \filldraw (11,0) circle (0.05);

    % timeline for radio
    \draw (-1.5, 1) node {RADIO};
    \filldraw (0,1) circle (0.05);
    \draw (0.5, 1.2) node {\tiny TX Delay};
    \draw (0,1) -- (1,1);
    \filldraw (1,1) circle (0.05);
    \draw (1.25, 0.8) node {\tiny Time of flight};
    \draw (1,1) -- (1.5,1);
    \filldraw (1.5,1) circle (0.05);
    \draw (2.0, 1.2) node {\tiny RX Delay};
    \draw (1.5,1) -- (2.5,1);
    \filldraw (2.5,1) circle (0.05);
  \end{tikzpicture}
  \label{fig:rx_tx_timing}
  \caption{Timing of radio and ultrasonic signals}
\end{figure}

\textbf{Path Loss} \\
We calculate the free space path loss (FSPL) at the farthest distance from the beacons. For this calculation, we assume the worst case where the beacon is on the other side of the field, which is $\SI{16.5}{\meter}$ away.
$$ \text{FSPL} = 20\log_{10}\Bigg(\frac{4\pi Rf^2}{c^2}\Bigg) = 20\log_{10}\Bigg(\frac{4\pi*16.5*\SI{433e6}{\hertz}}{\SI{3e8}{\meter\per\second}}\Bigg) = 49.521 $$
% TODO: explain the parts we know in detail, like the proposed beacon protocol

% TODO: Setup appendix for things like the survey
\section{Appenix}
\subsection{Appendix A}

  Survery Responses

  \begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{./survey_angle.png}
    \label{fig:survey_angle}
  \end{figure}

  \begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{./survey_position.png}
    \label{fig:survey_position}
  \end{figure}

  \begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{./survey_worth.png}
    \label{fig:survey_worth}
  \end{figure}


\bibliographystyle{plain}
\bibliography{phil-mqp}
\end{document}
